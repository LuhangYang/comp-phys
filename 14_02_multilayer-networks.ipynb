{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning in Multilayer Networks\n",
    "\n",
    "## Multi-layer perceptrons\n",
    "\n",
    "The multilayer perceptron is a feedforward network. It has input neurons, hidden neurons and output neurons. \n",
    "The hidden neurons may be arranged in a sequence of layers. The most common multilayer perceptrons have a single hidden layer, and are known as ‘two-layer’ networks, the number ‘two’ counting the number of layers of neurons not including the inputs.\n",
    "Such a feedforward network defines a nonlinear parameterized mapping from an input ${\\bf x}$ to an output ${\\bf y} = {\\bf y}({\\bf x};{\\bf w},{\\cal A})$. The output is a continuous function of the input and of the parameters ${\\bf w}$. The architecture of the network is referred-to as ${\\cal A}$. Feedforward networks can be ‘trained’ to perform regression and classification tasks.\n",
    "\n",
    "Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons or MLPs.\n",
    "\n",
    "### The network architecture\n",
    "\n",
    "The design of the input and output layers in a network is often straightforward. For example, suppose we are trying to determine whether a handwritten image depicts a \"9\" or not. A natural way to design the network is to encode the intensities of the image pixels into the input neurons. If the image is a $64 \\times 64$ greyscale image, then we'd have $4,096=64 \\times 64$\n",
    " input neurons, with the intensities scaled appropriately between 0 and 1.\n",
    "The output layer will contain just a single neuron, with output values of less than 0.5\n",
    " indicating \"input image is not a 9\", and values greater than 0.5\n",
    " indicating \"input image is a 9 \".\n",
    "\n",
    "While the design of the input and output layers of a neural network is often straightforward, there can be quite an art to the design of the hidden layers. In particular, it's not possible to sum up the design process for the hidden layers with a few simple rules of thumb. Instead, neural networks researchers have developed many design heuristics for the hidden layers, which help people get the behaviour they want out of their nets. For example, such heuristics can be used to help determine how to trade off the number of hidden layers against the time required to train the network. \n",
    "\n",
    "We have been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks. This means there are no loops in the network - information is always fed forward, never fed back. \n",
    "\n",
    "<img src=\"figures/narchitecture.png\" style=\"width: 500px;\"/>\n",
    "#### Basic network with two hidden layers\n",
    "\n",
    "### Regression networks\n",
    "In the case of a regression problem, the mapping for a network with one hidden layer may have the form:\n",
    "\n",
    "Hidden layer:\n",
    "$$\n",
    "a^{(1)}_j = \\sum_l w^{(1)}_{jl}x_l +\\theta_j^{(1)}; \\quad h_j =f^{(1)}(a_j^{(1)})\n",
    "$$\n",
    "\n",
    "Output layer:\n",
    "\n",
    "$$\n",
    "a^{(2)}_i = \\sum_j w^{(2)}_{ij}x_j +\\theta_i^{(2)}; \\quad y_i =f^{(2)}(a_i^{(2)})\n",
    "$$\n",
    "\n",
    "where, for example, $f^{(1)}(a) = \\tanh(a)$, and $f^{(2)}(a) = a$. Here $l$ runs over\n",
    "the inputs $x_1, \\cdots , x_L$, $j$ runs over the hidden units, and $i$ runs over the outputs. The ‘weights’ $w$ and ‘biases’ $\\theta$ together make up the parameter vector ${\\bf w}$.\n",
    "The nonlinear sigmoid function $f$ at the hidden layer gives the neural network greater computational flexibility than a standard linear regression model. Graphically, we can represent the neural network as a set of layers of connected neurons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How a regression network is traditionally trained\n",
    "\n",
    "This network is trained using a data set $D = ({{\\bf x}^{(n)}, {\\bf t}^{(n)}})$ by adjusting ${\\bf w}$ so as to minimize an error function, e.g.,\n",
    "\n",
    "$$\n",
    "E_D({\\bf w}) = \\sum_n\\sum_i (y_i({\\bf x}^{(n)};{\\bf w}) - t_i^{(n)})^2\n",
    "$$\n",
    "\n",
    "This objective function is a sum of terms, one for each input/target pair $\\{ {\\bf x}, {\\bf t} \\}$, measuring how close the output ${\\bf y}({\\bf x}; {\\bf w})$ is to the target ${\\bf t}$:\n",
    "\n",
    "$$\n",
    "E_D({\\bf w}) = \\sum_n E_{\\bf x}^{(n)}, \\quad E_{\\bf x}^{(n)}=\\sum_i (y_i({\\bf x}^{(n)};{\\bf w}) - t_i^{(n)})^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "This minimization is based on repeated evaluation of the gradient of $E_D$. This gradient can be efficiently computed using the backpropagation algorithm which uses the chain rule to find the derivatives, as we discuss below.\n",
    "\n",
    "Often, regularization (also known as weight decay) is included, modifying\n",
    "the objective function to:\n",
    "\n",
    "$$\n",
    "M({\\bf w})=\\alpha E_D({\\bf w}) + \\beta E_W({\\bf w}),\n",
    "$$\n",
    "where $E_W = \\frac{1}{2}\\sum_i w_i^2$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Gradient descent\n",
    "(From Wikipedia)\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point.\n",
    "\n",
    "Gradient descent is based on the observation that if the multi-variable function $ F(\\mathbf {x} )$ is defined and differentiable in a neighborhood of a point $ \\mathbf {a}$ , then $ F(\\mathbf {x} )$ decreases fastest if one goes from $ \\mathbf {a}$ in the direction of the negative gradient of $F$ at $ \\mathbf {a}$ , $ -\\nabla F(\\mathbf {a} )$. It follows that, if\n",
    "\n",
    "$$\\mathbf {a} _{n+1}=\\mathbf {a} _{n}-\\eta \\nabla F(\\mathbf {a} _{n})$$ \n",
    "\n",
    "for $\\eta$  small enough, then $F(\\mathbf {a_{n}} )\\geq F(\\mathbf {a_{n+1}} )$. In other words, the term $\\eta \\nabla F(\\mathbf {a} )$ is subtracted from $ \\mathbf {a}$  because we want to move against the gradient, namely down toward the minimum. With this observation in mind, one starts with a guess $\\mathbf {x} _{0}$ for a local minimum of $F$, and considers the sequence $\\mathbf {x} _{0},\\mathbf {x} _{1},\\mathbf {x} _{2},\\dots$  such that\n",
    "\n",
    "$${x} _{n+1}=\\mathbf {x} _{n}-\\gamma _{n}\\nabla F(\\mathbf {x} _{n}),\\ n\\geq 0.$$\n",
    "\n",
    "We have\n",
    "\n",
    "$F(\\mathbf {x} _{0})\\geq F(\\mathbf {x} _{1})\\geq F(\\mathbf {x} _{2})\\geq \\cdots$ ,\n",
    "so hopefully the sequence $(\\mathbf {x} _{n})$ converges to the desired local minimum. Note that the value of the step size $\\eta$  is allowed to change at every iteration.\n",
    "\n",
    "This process is illustrated in the adjacent picture. Here $F$ is assumed to be defined on the plane, and that its graph has a bowl shape. The blue curves are the contour lines, that is, the regions on which the value of $F$ is constant. A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point is orthogonal to the contour line going through that point. We see that gradient descent leads us to the bottom of the bowl, that is, to the point where the value of the function $F$ is minimal.\n",
    "\n",
    "<img src=\"figures/Gradient_descent.png\" style=\"width: 350px;\"/>\n",
    "#### Illustration of the gradient descept procedure on a series of iterations down a bowl shaped surface\n",
    "\n",
    "The \"Zig-Zagging\" nature of the method is also evident below, where the gradient descent method is applied to $$F(x,y)=\\sin \\left({\\frac {1}{2}}x^{2}-{\\frac {1}{4}}y^{2}+3\\right)\\cos(2x+1-e^{y})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3226181058 1.60235690268\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEPCAYAAACtCNj2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105e47e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "x0 = -1.4\n",
    "y0 = 0.5\n",
    "x = [x0] # The algorithm starts at x=6\n",
    "y = [y0] # The algorithm starts at x=6\n",
    "\n",
    "eta = 0.1 # step size multiplier\n",
    "precision = 0.00001\n",
    "\n",
    "def f(x,y):\n",
    "    f1 = x**2/2-y**2/4+3\n",
    "    f2 = 2*x+1-np.exp(y)\n",
    "    return np.sin(f1)*np.cos(f2)\n",
    "\n",
    "def gradf(x,y):\n",
    "    f1 = x**2/2-y**2/4+3\n",
    "    f2 = 2*x+1-np.exp(y)\n",
    "    dx = np.cos(f1)*np.cos(f2)*x-np.sin(f1)*np.sin(f2)*2.\n",
    "    dy = np.cos(f1)*np.cos(f2)*(-y/2.)-np.sin(f1)*np.sin(f2)*(-np.exp(y))\n",
    "    return (dx,dy)\n",
    "\n",
    "err = 100.\n",
    "while err > precision:\n",
    "    (step_x, step_y) = gradf(x0, y0)\n",
    "    x0 -= eta*step_x\n",
    "    y0 -= eta*step_y\n",
    "    x.append(x0)\n",
    "    y.append(y0)\n",
    "    err = eta*np.sqrt(step_x*step_x+step_y*step_y)\n",
    "\n",
    "\n",
    "print x0,y0\n",
    "\n",
    "#### All this below is just to visualize the process\n",
    "dx = 0.05\n",
    "dy = 0.05\n",
    "xx = np.arange(-1.5, 1.+dx, dx)\n",
    "yy = np.arange(0., 2.+dy, dy)\n",
    "V = np.zeros(shape=(len(yy),len(xx)))\n",
    "\n",
    "for iy in range(0,len(yy)):\n",
    "    for ix in range(0,len(xx)):\n",
    "        V[iy,ix] = f(xx[ix],yy[iy])\n",
    "\n",
    "X, Y = np.meshgrid(xx, yy)\n",
    "pyplot.contour(X, Y, V);\n",
    "\n",
    "pyplot.plot(x,y,linestyle='--', lw=3);\n",
    "pyplot.ylabel(\"y\")\n",
    "pyplot.xlabel(\"x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent (SGD)\n",
    "\n",
    "Stochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is a stochastic approximation of the gradient descent optimization and iterative method for minimizing an objective function that is written as a sum of differentiable functions. \n",
    "\n",
    "There are a number of challenges in applying the gradient descent rule. To understand what the problem is, let's look back at the quadratic cost $E_D$. Notice that this cost function has the form $E=\\sum_n E_{\\bf x}^{(n)}$\n",
    "In practice, to compute the gradient $\\nabla E_D$\n",
    " we need to compute the gradients $\\nabla E_{\\bf x}^{(n)}$\n",
    " separately for each training input, ${\\bf x^{(n)}}$\n",
    "and then average them.\n",
    ". Unfortunately, when the number of training inputs is very large this can take a long time, and learning thus occurs slowly.\n",
    "\n",
    "Stochastic gradient descent can be used to speed up learning. The idea is to estimate the gradient $\\nabla E$\n",
    " by computing $\\nabla E_{\\bf x}$\n",
    "for a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient.\n",
    "\n",
    "<!--To make these ideas more precise, stochastic gradient descent works by randomly picking out a small number $m$\n",
    " of randomly chosen training inputs. We'll label those random training inputs ${\\bf x^{(1)},x^{(2)},…,x^{(m)}}$\n",
    ", and refer to them as a mini-batch. Provided the sample size m\n",
    " is large enough we expect that the average value of the $\\nabla E_x$\n",
    " will be roughly equal to the average over all of them, that is\n",
    "$$\\frac{1}{m}\\sum _{j=1}^m \\nabla E_{x^{j}} \\approx \\frac{1}{n}\\sum _{j=1}^n \\nabla E_{x^{j}}$$\n",
    "where the second sum is over the entire set of training data. \n",
    "!-->\n",
    "\n",
    "To connect this explicitly to learning in neural networks, suppose $w_k$\n",
    " and $b_l$\n",
    " denote the weights and biases in our neural network. Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those,\n",
    "$$\n",
    "w_k \\rightarrow w_k - \\eta \\sum_{j=1}^m \\frac{\\partial{E_{\\bf x}^{(j)}}}{\\partial w_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_l \\rightarrow b_l - \\eta \\sum_{j=1}^m \\frac{\\partial{E_{\\bf x}^{(j)}}}{\\partial b_l}\n",
    "$$\n",
    "\n",
    "where the sums are over all the training examples in the current mini-batch. Then we pick out another randomly chosen mini-batch and train with those. And so on, until we have exhausted the training inputs, which is said to complete an epoch of training. At that point we start over with a new training epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A simple network to classify handwritten digits\n",
    "Most of this section has been taken from M. Nielsen's free on-line book: \"Neural Networks and Deep Learning\" https://help.github.com/articles/basic-writing-and-formatting-syntax/#quoting-code\n",
    "\n",
    "In this section we discuss a neural network which can solve the more interesting and difficult problem, namely, recognizing individual handwritten digits.\n",
    "\n",
    "The input layer of the network contains neurons encoding the values of the input pixels. As discussed in the next section, our training data for the network will consist of many 28 by 28\n",
    " pixel images of scanned handwritten digits, and so the input layer contains 784=28×28\n",
    " neurons. The input pixels are greyscale, with a value of 0.0\n",
    " representing white, a value of 1.0\n",
    "representing black, and in between values representing gradually darkening shades of grey.\n",
    "\n",
    "The second layer of the network is a hidden layer. We denote the number of neurons in this hidden layer by $n$\n",
    ", and we'll experiment with different values for $n$\n",
    ". The example shown illustrates a small hidden layer, containing just $n=15$\n",
    " neurons.\n",
    " \n",
    " The output layer of the network contains 10 neurons. If the first neuron fires, i.e., has an output $\\sim 1$\n",
    ", then that will indicate that the network thinks the digit is a 0\n",
    ". If the second neuron fires then that will indicate that the network thinks the digit is a 1\n",
    ". And so on. A little more precisely, we number the output neurons from 0\n",
    " through 9\n",
    ", and figure out which neuron has the highest activation value. If that neuron is, say, neuron number 6\n",
    ", then our network will guess that the input digit was a 6\n",
    ". And so on for the other output neurons.\n",
    "\n",
    "<img src=\"figures/nnetwork.png\" style=\"width: 500px;\"/>\n",
    "#### Network to identify single digits. The output layer has 10 neurons, one for each digit.\n",
    "\n",
    "\n",
    "\n",
    "The first thing we'll need is a data set to learn from - a so-called training data set. We'll use the MNIST data set, which contains tens of thousands of scanned images of handwritten digits, together with their correct classifications. MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology. Here's a few images from MNIST:\n",
    "\n",
    "<img src=\"figures/digits_separate.png\" style=\"width: 250px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "The MNIST data comes in two parts. The first part contains 60,000 images to be used as training data. These images are scanned handwriting samples from 250 people, half of whom were US Census Bureau employees, and half of whom were high school students. The images are greyscale and 28 by 28 pixels in size. The second part of the MNIST data set is 10,000 images to be used as test data. Again, these are 28 by 28 greyscale images. We'll use the test data to evaluate how well our neural network has learned to recognize digits. To make this a good test of performance, the test data was taken from a different set of 250 people than the original training data (albeit still a group split between Census Bureau employees and high school students). This helps give us confidence that our system can recognize digits from people whose writing it didn't see during training.\n",
    "\n",
    "We'll use the notation $x$\n",
    " to denote a training input. It'll be convenient to regard each training input $x$\n",
    " as a 28×28=784-dimensional vector. Each entry in the vector represents the grey value for a single pixel in the image. We'll denote the corresponding desired output by y=y(x)\n",
    ", where y\n",
    " is a 10\n",
    "-dimensional vector. For example, if a particular training image, $x$\n",
    ", depicts a 6\n",
    ", then $y(x)=(0,0,0,0,0,0,1,0,0,0)^T$\n",
    " is the desired output from the network. Note that T\n",
    " here is the transpose operation, turning a row vector into an ordinary (column) vector.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import cPickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the biases and weights are stored as lists of Numpy matrices. So, for example `net.weights[1]` is a Numpy matrix storing the weights connecting the second and third layers of neurons. (It's not the first and second layers, since Python's list indexing starts at 0.) Since `net.weights[1]` is rather verbose, let's just denote that matrix $w$\n",
    ". It's a matrix such that $w_{jk}$\n",
    " is the weight for the connection between the $k^{th}$\n",
    " neuron in the second layer, and the $j^{th}$\n",
    " neuron in the third layer. This ordering of the $j$\n",
    " and $k$\n",
    " indices may seem strange. The big advantage of using this ordering is that it means that the vector of activations of the third layer of neurons is:\n",
    "$$a′=\\mathrm {sigmoid}(wa+b)$$\n",
    "\n",
    "There's quite a bit going on in this equation, so let's unpack it piece by piece. $a$\n",
    " is the vector of activations of the second layer of neurons. To obtain $a′$\n",
    " we multiply $a$\n",
    " by the weight matrix $w$\n",
    ", and add the vector $b$\n",
    " of biases. We then apply the function `sigmoid`\n",
    "elementwise to every entry in the vector $wa+b$.\n",
    "\n",
    "Of course, the main thing we want our Network objects to do is to learn. To that end we'll give them an SGD method which implements stochastic gradient descent. \n",
    "\n",
    "The training_data is a list of tuples `(x, y)` representing the training inputs and corresponding desired outputs. The variables `epochs` and `mini_batch_size` are what you'd expect - the number of epochs to train for, and the size of the mini-batches to use when sampling. `eta` is the learning rate, $\\eta$. If the optional argument `test_data` is supplied, then the program will evaluate the network after each epoch of training, and print out partial progress. This is useful for tracking progress, but slows things down substantially.\n",
    "\n",
    "The code works as follows. In each epoch, it starts by randomly shuffling the training data, and then partitions it into mini-batches of the appropriate size. This is an easy way of sampling randomly from the training data. Then for each `mini_batch` we apply a single step of gradient descent. This is done by the code `self.update_mini_batch(mini_batch, eta)`, which updates the network weights and biases according to a single iteration of gradient descent, using just the training data in `mini_batch`.\n",
    "\n",
    "Most of the work is done by the line\n",
    "```\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "```\n",
    "\n",
    "This invokes something called the *backpropagation* algorithm, which is a fast way of computing the gradient of the cost function. So `update_mini_batch` works simply by computing these gradients for every training example in the `mini_batch`, and then updating `self.weights` and `self.biases` appropriately.\n",
    "\n",
    "Apart from `self.backprop` the program is self-explanatory - all the heavy lifting is done in `self.SGD` and `self.update_mini_batch`, which we've already discussed. The `self.backprop` method makes use of a few extra functions to help in computing the gradient, namely sigmoid_prime, which computes the derivative of the sigmoid\n",
    " function, and `self.cost_derivative`. You can get the gist of these (and perhaps the details) just by looking at the code and documentation strings. Note that while the program appears lengthy, much of the code is documentation strings intended to make the code easy to understand. In fact, the program contains just 74 lines of non-whitespace, non-comment code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9039 / 10000\n",
      "Epoch 1: 9267 / 10000\n",
      "Epoch 2: 9290 / 10000\n",
      "Epoch 3: 9362 / 10000\n",
      "Epoch 4: 9410 / 10000\n",
      "Epoch 5: 9456 / 10000\n",
      "Epoch 6: 9431 / 10000\n",
      "Epoch 7: 9461 / 10000\n",
      "Epoch 8: 9489 / 10000\n",
      "Epoch 9: 9475 / 10000\n",
      "Epoch 10: 9512 / 10000\n",
      "Epoch 11: 9514 / 10000\n",
      "Epoch 12: 9509 / 10000\n",
      "Epoch 13: 9498 / 10000\n",
      "Epoch 14: 9505 / 10000\n",
      "Epoch 15: 9529 / 10000\n",
      "Epoch 16: 9516 / 10000\n",
      "Epoch 17: 9536 / 10000\n",
      "Epoch 18: 9512 / 10000\n",
      "Epoch 19: 9496 / 10000\n",
      "Epoch 20: 9525 / 10000\n",
      "Epoch 21: 9536 / 10000\n",
      "Epoch 22: 9533 / 10000\n",
      "Epoch 23: 9528 / 10000\n",
      "Epoch 24: 9518 / 10000\n",
      "Epoch 25: 9511 / 10000\n",
      "Epoch 26: 9554 / 10000\n",
      "Epoch 27: 9528 / 10000\n",
      "Epoch 28: 9533 / 10000\n",
      "Epoch 29: 9543 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Challenge 14.1\n",
    "Try creating a network with just two layers - an input and an output layer, no hidden layer - with 784 and 10 neurons, respectively. Train the network using stochastic gradient descent. What classification accuracy can you achieve?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
